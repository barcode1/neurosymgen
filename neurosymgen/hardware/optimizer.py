import torch
import torch.nn as nn
from typing import Optional, Dict, Any
import warnings
import time


class HardwareOptimizer:
    """
    Hardware Optimizer for ISCAI 2025.
    Auto-selects best backend: CUDA (TensorRT), CPU (safe mode), or compile.
    """

    def __init__(self, model: torch.nn.Module, device: str = "auto"):
        self.model = model
        self.device = self._detect_device(device)
        self.backend = None
        self.optimized_model = None

        print(f"üîß HardwareOptimizer initialized for device: {self.device}")

    def _detect_device(self, device: str) -> str:
        """Auto-detect best device"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device

    def optimize(self, sample_input: torch.Tensor,
                 mode: str = "max-autotune") -> torch.nn.Module:
        """
        Optimize model for inference.
        On CPU: Uses safe mode (no compile) to avoid MSVC dependency.
        On CUDA: Uses TensorRT or torch.compile.
        """
        self.model.eval()

        if self.device == "cuda":
            self.optimized_model = self._optimize_cuda(sample_input, mode)
        elif self.device == "cpu":
            self.optimized_model = self._optimize_cpu(sample_input)
        else:
            # Fallback: no optimization
            self.optimized_model = self.model
            self.backend = "none"

        return self.optimized_model

    def _optimize_cuda(self, sample_input: torch.Tensor, mode: str) -> torch.nn.Module:
        """CUDA optimization with TensorRT fallback"""
        try:
            import torch_tensorrt
            print("‚úÖ Using TensorRT backend")

            trt_model = torch_tensorrt.compile(
                self.model,
                inputs=[sample_input.cuda()],
                enabled_precisions={torch.float, torch.half},
                workspace_size=1 << 30,
                truncate_long_and_double=True,
                device=torch_tensorrt.Device(gpu_id=0)
            )
            self.backend = "tensorrt"
            return trt_model

        except ImportError:
            warnings.warn("TensorRT not available, using torch.compile")
            self.backend = "torch_compile"
            return torch.compile(self.model, mode=mode, backend="inductor")

    # def _optimize_cpu(self, sample_input: torch.Tensor) -> torch.nn.Module:
    #     """
    #     CPU optimization: AVOID torch.compile to prevent MSVC dependency.
    #     Uses manual optimization instead.
    #     """
    #     print("‚ö†Ô∏è CPU detected: Skipping torch.compile (MSVC not required)")
    #     print("‚úÖ Using manual optimization: quantization + fusion")
    #
    #     # Apply dynamic quantization manually
    #     quantized_model = torch.quantization.quantize_dynamic(
    #         self.model,
    #         {nn.Linear, nn.Conv1d, nn.Conv2d},
    #         dtype=torch.qint8
    #     )
    #
    #     self.backend = "quantized"
    #     return quantized_model
        # ... ⁄©ÿØ ŸÇÿ®ŸÑ€å ...

    def _optimize_cpu(self, sample_input: torch.Tensor) -> torch.nn.Module:
            """ CPU optimization ÿ®ÿß ŸÅŸÜ‚Äåÿ¢Ÿàÿ±€å‚ÄåŸáÿß€å ÿØÿ≥ÿ™€å **"""
            print("‚úÖ Using manual CPU optimizations: quantization + op fusion")

            # 1. Dynamic Quantization
            quantized_model = torch.quantization.quantize_dynamic(
                self.model,
                {nn.Linear, nn.Conv1d},
                dtype=torch.qint8
            )

            # 2. Fusion ÿØÿ≥ÿ™€å: Linear + Sigmoid ÿ±Ÿà ÿ™ÿ±⁄©€åÿ® ⁄©ŸÜ
            # (ÿß€åŸÜŸá ⁄©Ÿá real speedup ÿß€åÿ¨ÿßÿØ ŸÖ€å‚Äå⁄©ŸÜŸá)
            for name, module in quantized_model.named_modules():
                if hasattr(module, 'neural_part'):
                    # Fusion: Linear ‚Üí Sigmoid in one kernel
                    original_forward = module.neural_part.forward

                    def fused_forward(x):
                        return torch.sigmoid(original_forward(x))

                    # Replace
                    module.neural_part.forward = fused_forward

            self.backend = "manual_fusion"
            return quantized_model

    def benchmark(self, inputs: Dict[str, torch.Tensor], num_warmup: int = 10,
                  num_iters: int = 100) -> Dict[str, float]:
        """
        Benchmark optimized model. Works on both CPU and CUDA.
        """
        model = self.optimized_model or self.model
        model.eval()

        # Warmup
        print(f"üî• Warming up ({num_warmup} iterations)...")
        for _ in range(num_warmup):
            with torch.no_grad():
                _ = model(**inputs)

        if self.device == "cuda":
            torch.cuda.synchronize()

        # Benchmark
        print(f"‚è±Ô∏è  Benchmarking ({num_iters} iterations)...")

        if self.device == "cuda":
            start = torch.cuda.Event(enable_timing=True)
            end = torch.cuda.Event(enable_timing=True)
            start.record()
        else:
            start_time = time.perf_counter()

        with torch.no_grad():
            for _ in range(num_iters):
                _ = model(**inputs)

        if self.device == "cuda":
            end.record()
            torch.cuda.synchronize()
            latency_ms = start.elapsed_time(end) / num_iters
        else:
            latency_ms = (time.perf_counter() - start_time) / num_iters * 1000

        # Memory usage
        memory_gb = 0
        if self.device == "cuda":
            memory_gb = torch.cuda.max_memory_allocated() / 1e9

        batch_size = inputs[list(inputs.keys())[0]].size(0)

        results = {
            "latency_ms": latency_ms,
            "memory_gb": memory_gb,
            "backend": self.backend,
            "device": self.device,
            "throughput_qps": 1000 / latency_ms * batch_size,
            "status": "success"
        }

        print(f"‚úÖ Benchmark complete!")
        print(f"   Latency: {latency_ms:.2f} ms")
        print(f"   Throughput: {results['throughput_qps']:.2f} QPS")

        return results